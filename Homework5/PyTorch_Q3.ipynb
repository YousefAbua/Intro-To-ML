{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF8Z6B/C8bLAU0lP83vajT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousefAbua/Intro-To-ML/blob/main/Homework5/PyTorch_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxVdBz2tgh7x",
        "outputId": "a5dbcc9d-3e1a-4f08-8b31-41e88890db6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and read file\n",
        "file_path = '/content/drive/My Drive/Intro to ML/Datasets/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))\n",
        "\n",
        "varlist = ['area','bedrooms','bathrooms','stories','parking', 'price']\n",
        "\n",
        "charlist = ['mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea']\n",
        "\n",
        "def binary_map(x):\n",
        "  return x.map({'yes' : 1, 'no' : 0})\n",
        "\n",
        "# Applying binary map\n",
        "housing[charlist] = housing[charlist].apply(binary_map)\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "housing[varlist] = scaler.fit_transform(housing[varlist])\n",
        "\n",
        "housing.pop('furnishingstatus')\n",
        "x = housing.values\n",
        "y = housing.pop('price')"
      ],
      "metadata": {
        "id": "FR0Pp5xFgq6o"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)"
      ],
      "metadata": {
        "id": "YFNOWbzuiG8r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = x.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]"
      ],
      "metadata": {
        "id": "WawjBUAkiKeK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x[train_indices]\n",
        "y_train = y[train_indices]\n",
        "\n",
        "x_test = x[val_indices]\n",
        "y_test = y[val_indices]\n",
        "\n",
        "xn_train = 0.1 * x_train\n",
        "xn_test = 0.1 * x_test"
      ],
      "metadata": {
        "id": "1fCKEHe-iMNO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, b):\n",
        "  return w1 * x[:,0] + w2 * x[:,1] + w3 * x[:,2] + w4 * x[:,3] + w5 * x[:,4] + w6 * x[:,5] + w7 * x[:,6] + w8 * x[:,7] + w9 * x[:,8] + w10 * x[:,9] + b\n",
        "\n",
        "def loss_fn(predict, y):\n",
        "  squared_diffs = (predict - y)**2\n",
        "  return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "x61ebiNAiRh4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, x_train, x_test, y_train, y_test):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "      train_predict = model(x_train, *params)\n",
        "      train_loss = loss_fn(train_predict, y_train)\n",
        "\n",
        "      val_predict = model(x_test, *params)\n",
        "      val_loss = loss_fn(val_predict, y_test)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 500 == 0:\n",
        "          print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "                f\" Validation loss {val_loss.item():.4f}\")\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "l-Uyq9GAiS5t"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"Learning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    x_train = x_train,\n",
        "    x_test = x_test,\n",
        "    y_train = y_train,\n",
        "    y_test = y_test)\n",
        "\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    x_train = x_train,\n",
        "    x_test = x_test,\n",
        "    y_train = y_train,\n",
        "    y_test = y_test)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    x_train = x_train,\n",
        "    x_test = x_test,\n",
        "    y_train = y_train,\n",
        "    y_test = y_test)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    x_train = x_train,\n",
        "    x_test = x_test,\n",
        "    y_train = y_train,\n",
        "    y_test = y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVbCI_npiUBJ",
        "outputId": "c73defaf-17d7-4d79-f94c-d3ae4a58da75"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate:  0.1  SGD OPTIMIZER\n",
            "Epoch 500, Training loss 0.0024, Validation loss 0.0033\n",
            "Epoch 1000, Training loss 0.0006, Validation loss 0.0008\n",
            "Epoch 1500, Training loss 0.0001, Validation loss 0.0002\n",
            "Epoch 2000, Training loss 0.0000, Validation loss 0.0001\n",
            "Epoch 2500, Training loss 0.0000, Validation loss 0.0000\n",
            "Epoch 3000, Training loss 0.0000, Validation loss 0.0000\n",
            "Epoch 3500, Training loss 0.0000, Validation loss 0.0000\n",
            "Epoch 4000, Training loss 0.0000, Validation loss 0.0000\n",
            "Epoch 4500, Training loss 0.0000, Validation loss 0.0000\n",
            "Epoch 5000, Training loss 0.0000, Validation loss 0.0000\n",
            "\n",
            "\n",
            "Learning Rate:  0.01  SGD OPTIMIZER\n",
            "Epoch 500, Training loss 0.0560, Validation loss 0.0822\n",
            "Epoch 1000, Training loss 0.0238, Validation loss 0.0394\n",
            "Epoch 1500, Training loss 0.0135, Validation loss 0.0221\n",
            "Epoch 2000, Training loss 0.0089, Validation loss 0.0139\n",
            "Epoch 2500, Training loss 0.0064, Validation loss 0.0096\n",
            "Epoch 3000, Training loss 0.0050, Validation loss 0.0072\n",
            "Epoch 3500, Training loss 0.0040, Validation loss 0.0057\n",
            "Epoch 4000, Training loss 0.0033, Validation loss 0.0046\n",
            "Epoch 4500, Training loss 0.0028, Validation loss 0.0039\n",
            "Epoch 5000, Training loss 0.0024, Validation loss 0.0033\n",
            "\n",
            "\n",
            "Learning Rate:  0.001  SGD OPTIMIZER\n",
            "Epoch 500, Training loss 0.4168, Validation loss 0.4235\n",
            "Epoch 1000, Training loss 0.2609, Validation loss 0.2735\n",
            "Epoch 1500, Training loss 0.1962, Validation loss 0.2149\n",
            "Epoch 2000, Training loss 0.1524, Validation loss 0.1754\n",
            "Epoch 2500, Training loss 0.1219, Validation loss 0.1476\n",
            "Epoch 3000, Training loss 0.1002, Validation loss 0.1272\n",
            "Epoch 3500, Training loss 0.0843, Validation loss 0.1119\n",
            "Epoch 4000, Training loss 0.0724, Validation loss 0.0999\n",
            "Epoch 4500, Training loss 0.0632, Validation loss 0.0902\n",
            "Epoch 5000, Training loss 0.0559, Validation loss 0.0822\n",
            "\n",
            "\n",
            "Learning Rate:  0.0001  SGD OPTIMIZER\n",
            "Epoch 500, Training loss 5.7477, Validation loss 5.5746\n",
            "Epoch 1000, Training loss 3.6718, Validation loss 3.5723\n",
            "Epoch 1500, Training loss 2.4022, Validation loss 2.3467\n",
            "Epoch 2000, Training loss 1.6236, Validation loss 1.5944\n",
            "Epoch 2500, Training loss 1.1443, Validation loss 1.1306\n",
            "Epoch 3000, Training loss 0.8473, Validation loss 0.8427\n",
            "Epoch 3500, Training loss 0.6616, Validation loss 0.6623\n",
            "Epoch 4000, Training loss 0.5438, Validation loss 0.5476\n",
            "Epoch 4500, Training loss 0.4675, Validation loss 0.4731\n",
            "Epoch 5000, Training loss 0.4167, Validation loss 0.4235\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6636,  0.7277,  0.5722,  0.8491,  0.6190,  0.0891,  0.6279,  0.4686,\n",
              "         0.9407,  0.4551, -0.9524], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBUsE077iVLh"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}