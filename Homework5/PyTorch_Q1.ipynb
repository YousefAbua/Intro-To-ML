{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPntUbsNHfcRCS6jgm+r0WX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousefAbua/Intro-To-ML/blob/main/Homework5/PyTorch_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "id": "1xuZMJUXOiHm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ],
      "metadata": {
        "id": "rBr9oD06QKsC"
      },
      "execution_count": 421,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(t_u, w1, w2, b):\n",
        "  return w2*t_u**2 + w1*t_u + b"
      ],
      "metadata": {
        "id": "VXJjNswOQskH"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "5EpvTlO4Q3k5"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "Iwg6_0jcgB-d"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"Learning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.01 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.01 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.01 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" SGD OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.01 * t_u,\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLUHjGT4g2KS",
        "outputId": "22d9908d-9158-47fc-e361-fa632db8b484"
      },
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate:  0.1  SGD OPTIMIZER\n",
            "Epoch 500, Loss 2.108456\n",
            "Epoch 1000, Loss 2.090728\n",
            "Epoch 1500, Loss 2.090720\n",
            "Epoch 2000, Loss 2.090720\n",
            "Epoch 2500, Loss 2.090721\n",
            "Epoch 3000, Loss 2.090719\n",
            "Epoch 3500, Loss 2.090720\n",
            "Epoch 4000, Loss 2.090720\n",
            "Epoch 4500, Loss 2.090720\n",
            "Epoch 5000, Loss 2.090720\n",
            "\n",
            "\n",
            "Learning Rate:  0.01  SGD OPTIMIZER\n",
            "Epoch 500, Loss 26.266186\n",
            "Epoch 1000, Loss 12.949670\n",
            "Epoch 1500, Loss 6.968258\n",
            "Epoch 2000, Loss 4.281574\n",
            "Epoch 2500, Loss 3.074787\n",
            "Epoch 3000, Loss 2.532740\n",
            "Epoch 3500, Loss 2.289264\n",
            "Epoch 4000, Loss 2.179901\n",
            "Epoch 4500, Loss 2.130778\n",
            "Epoch 5000, Loss 2.108713\n",
            "\n",
            "\n",
            "Learning Rate:  0.001  SGD OPTIMIZER\n",
            "Epoch 500, Loss 58.822189\n",
            "Epoch 1000, Loss 48.348232\n",
            "Epoch 1500, Loss 44.396774\n",
            "Epoch 2000, Loss 41.118404\n",
            "Epoch 2500, Loss 38.115955\n",
            "Epoch 3000, Loss 35.345898\n",
            "Epoch 3500, Loss 32.788918\n",
            "Epoch 4000, Loss 30.428556\n",
            "Epoch 4500, Loss 28.249678\n",
            "Epoch 5000, Loss 26.238340\n",
            "\n",
            "\n",
            "Learning Rate:  0.0001  SGD OPTIMIZER\n",
            "Epoch 500, Loss 138.825058\n",
            "Epoch 1000, Loss 118.428955\n",
            "Epoch 1500, Loss 102.817139\n",
            "Epoch 2000, Loss 90.845161\n",
            "Epoch 2500, Loss 81.642403\n",
            "Epoch 3000, Loss 74.546761\n",
            "Epoch 3500, Loss 69.054512\n",
            "Epoch 4000, Loss 64.782639\n",
            "Epoch 4500, Loss 61.439720\n",
            "Epoch 5000, Loss 58.804115\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.8014, 3.7440, 4.9612], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 425
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.Adam([params], lr=learning_rate)\n",
        "print(\"Learning Rate: \", learning_rate, \" ADAM OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.1 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" ADAM OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.1 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" ADAM OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.1 * t_u,\n",
        "    t_c = t_c)\n",
        "\n",
        "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam([params], lr=learning_rate)\n",
        "print(\"\\n\\nLearning Rate: \", learning_rate, \" ADAM OPTIMIZER\")\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = 0.1 * t_u,\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3uY3Q_VhNc9",
        "outputId": "13b47cf6-3fd3-40e0-b78c-54291cba8ace"
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate:  0.1  ADAM OPTIMIZER\n",
            "Epoch 500, Loss 2.782507\n",
            "Epoch 1000, Loss 2.486027\n",
            "Epoch 1500, Loss 2.261513\n",
            "Epoch 2000, Loss 2.144077\n",
            "Epoch 2500, Loss 2.101927\n",
            "Epoch 3000, Loss 2.092149\n",
            "Epoch 3500, Loss 2.090817\n",
            "Epoch 4000, Loss 2.090725\n",
            "Epoch 4500, Loss 2.090722\n",
            "Epoch 5000, Loss 2.090721\n",
            "\n",
            "\n",
            "Learning Rate:  0.01  ADAM OPTIMIZER\n",
            "Epoch 500, Loss 6.111171\n",
            "Epoch 1000, Loss 3.936777\n",
            "Epoch 1500, Loss 3.117804\n",
            "Epoch 2000, Loss 2.931840\n",
            "Epoch 2500, Loss 2.871260\n",
            "Epoch 3000, Loss 2.812938\n",
            "Epoch 3500, Loss 2.744088\n",
            "Epoch 4000, Loss 2.664673\n",
            "Epoch 4500, Loss 2.576367\n",
            "Epoch 5000, Loss 2.482456\n",
            "\n",
            "\n",
            "Learning Rate:  0.001  ADAM OPTIMIZER\n",
            "Epoch 500, Loss 103.795021\n",
            "Epoch 1000, Loss 13.018479\n",
            "Epoch 1500, Loss 8.064860\n",
            "Epoch 2000, Loss 7.688990\n",
            "Epoch 2500, Loss 7.295183\n",
            "Epoch 3000, Loss 6.830939\n",
            "Epoch 3500, Loss 6.306171\n",
            "Epoch 4000, Loss 5.739597\n",
            "Epoch 4500, Loss 5.159210\n",
            "Epoch 5000, Loss 4.600074\n",
            "\n",
            "\n",
            "Learning Rate:  0.0001  ADAM OPTIMIZER\n",
            "Epoch 500, Loss 578.252686\n",
            "Epoch 1000, Loss 491.236542\n",
            "Epoch 1500, Loss 413.867676\n",
            "Epoch 2000, Loss 345.253937\n",
            "Epoch 2500, Loss 284.667236\n",
            "Epoch 3000, Loss 231.510590\n",
            "Epoch 3500, Loss 185.283310\n",
            "Epoch 4000, Loss 145.552078\n",
            "Epoch 4500, Loss 111.921623\n",
            "Epoch 5000, Loss 84.009270\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.5698,  0.5721, -0.4337], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 426
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0JDSlBz19rU"
      },
      "execution_count": 426,
      "outputs": []
    }
  ]
}